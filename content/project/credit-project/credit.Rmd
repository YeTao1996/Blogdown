---
title: Default Detection
author: Ye Tao(Janson)
date: '2020-01-05'
slug: test
categories: ["R"]
tags: [R]
subtitle: ''
summary: 'Credit Default Prject'
authors: [Ye Tao(Janson)]
lastmod: '2020-01-05T20:22:01-08:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
output: 
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, 
                      message = FALSE, fig.height = 5, fig.width = 10)
```


```{r, warning=FALSE, message=FALSE}
# loading Library
library(tidyverse)
library(ggcorrplot)
library(GGally)
library(skimr)
library(corrplot)
library(pROC)
library(ggridges)
library(caret)
library(Rtsne)
library(doParallel)
library(rpart.plot)
library(rpart)
theme_set(theme_bw())
```

# **Introduction**

**`Fraud`** attempts have seen a drastic increase in recent years, making fraud detection is more important than ever. For instance, recent research shows a typical organization loses 5% of revenues each year because of fraud with a median loss of $145,000 per case. Consequently in this project, we’re interested in **`Fraud Detection`**, and we will figure out an ML-based way for recognizing fraud activities with several features given. Notice that this should be a classification problem.

In general, this project will be composed of 4 parts: 


- Data Exploration & Data Preprocessing 
- Feature Selection & Dimension Reduction
- Modeling with various techniques: Xgboost and Random Forest Logistic regression, Decision tree
- Model Performance Evaluation & Comparison

<br>

Key questions about the dataset:

- How are a fraud and non-fraud classes different from each other?
- When does fraud events tend to occur?
- Which variable has the most contribution to our model?
- What is the best model among these 4 modeling methodologies(model comparison)?



```{r}
#Laptop
#bdata = readr::read_csv("c://Users//BOSS/Desktop/creditcard.csv")

# Workstation
data = readr::read_csv("C://Users//yetao//OneDrive//Desktop//creditcard.csv")
data$Class = as.factor(data$Class)
levels(data$Class) <- c("Not_Default", "Default")
```

# **Data Overview**

```{r}
# glimpse the data
knitr::kable(skim(data), format = "markdown")
```



<br>

# **Data Exploration**

## **Data Imbalance Check**

As we can see from the table below, This dataset has 284315 legitimate credit card transactions and only 492 problematic transactions. This is Data Imbalance and it is a common situation we need to deal with credit default detection projects.

Because the dataset is extremely unbalanced. Even a Dumb classifier, which simply uses the frequency for prediction, would obtain around 99% accuracy on this task. This is clear that a simple measure of mean accuracy should not be used due to insensitivity to false negatives. However, the good news is that there’s no missing value in this dataset.

```{r}
class.number <- data %>% 
  group_by(Class) %>% 
  summarise(n = n())

# Imbalanced Data Checking
ggplot(data = class.number, aes(x = Class, y = n, fill = Class)) +
    geom_bar(alpha = 0.4, stat = "identity") +
    geom_text(aes(label = n)) + 
    ggtitle("Bar Plot for Non-Default and Default") +
    ylab("Number of transactions") +
    scale_fill_discrete(name = "Class Type", labels = c("Non-Default", "Default")) + 
    theme(legend.position = "bottom", legend.box = "horizontal")
```

Notice that since only `492` of all transactions are fraudulent, always predicting that a transaction is not fraudulent would lead to a low error rate. It is important to construct a model that performs better than this. In other words, training a Machine Learning Model with this imbalanced dataset, often causes the model to develop a certain bias towards the majority class. Also, we have to keep in mind that accuracy is no more important, but other criteria like specificity, ROC score, and many others needed to be calculated in this projection for the purpose of comparison.
<br>

## **Explore Difference betweem Default and Non-default class In Amount and Time**

Recall that for privacy issues, v1 to v28 is all PCA transformed, which means that we have no idea what they should be. However, time and amount have physical meanings and therefore we can put particular attention on these two variables.

### **Explore feature - Time**

As the plot below suggests, we can see some differences between default and non-default transactions according to the time feature. For two shaded regions in the plot, we notice that the amount of innocent transactions within that two parts goes down. While the default one stays at a high position. This is probably because people who do non-fraudulent transactions are having sleep during those two periods of time. While the problematic transaction is more likely to occur during the night since crime might feel afraid to be caught while using fake credit cards during the daytime.

```{r}
# Density plot with time(0 & 1)
p1 = ggplot(data = data, aes(x = Time, fill = Class)) +
  geom_density(alpha = 0.3) +
  scale_fill_discrete(name = "Class Type", labels = c("Non-Default", "Default")) + 
  ggtitle("Density Plot - Variable Time (Fraud VS Non-Fraud)") + 
  theme(legend.position = "bottom", legend.box = "horizontal")

p1 +
  annotate("rect", xmin = 0.95e5, xmax = 1.1e+5, ymin = 1e-06, ymax = 1.9e-06,
  alpha = .4) +
  annotate("rect", xmin = 1.2e4, xmax = 2.3e+4, ymin = 1e-06, ymax = 1.9e-06,
  alpha = .4)
```

<br>

Notice that the `Time` variable is not very significant is our model since it measures the seconds elapsed between each transaction and the first transaction in the dataset. It can not give us too much useful information, therefore we discard this variable in the following modeling parts.

### **Explore Feature - Amount**

```{r}
# Density plot with time(0 & 1)
p2 = ggplot(data = data, aes(x = Amount, fill = Class)) + 
  geom_density(alpha = 0.3) +
  scale_fill_discrete(name = "Class Type", labels = c("Non-Default", "Default")) + 
  ggtitle("Density Plot - Variable Amount (Fraud VS Non-Fraud)") + 
  theme(legend.position = "bottom", legend.box = "horizontal") + 
  scale_x_log10() + 
  xlim(0, 200) + ylim(0, 0.05)
p2
```

The density plot shows the right skewness since most credit card transactions should be around a smaller amount of money. For example, buying a bottle of water or some sandwiches on the campus, and a small number of such people will use a credit card to buy some luxury goods. In addition to that, as the graph suggests above, generally speaking, the amount for default transaction is lower than the non default one. However, when the amount is around 100, the amount for the default part is much higher. So, we might conclude that default is more likely to occur when the amount is `$100`.

### **Explore Remaining Features**

In this section, we plot the distribution of every single feature corresponding to its distribution. Here are the criteria: If two distribution overlapped each other for a large part, that feature may not be helpful in distinguishing default and non-default. On the other hand, if two distributions separated from each other, the corresponding feature is good to include inside the model. However, we have to keep in mind that this part only gives us a general overview on which features might be significant. We will further check this importance of variable later by using `Random Forest` to calculate the importance of each feature.

```{r,  fig.height = 5, fig.width = 10}
data %>%
  select(V1:V14, Class) %>% 
  gather(A, B, -Class) %>% 
  ggplot(aes(x = B, y = A, fill = Class)) + 
  geom_density_ridges(alpha = 0.5) +
  scale_x_log10() + 
  ggtitle("Density Plot for each Variable") + 
  xlab("Value") +
  ylab("Variable Name")
```

According to the plot above, from v1 to v14, we can see two distributions of following features which somehow
overlapped: `v1`, `v10`, `v12`, `v13`, `v14`, `v3`, `v5`, `v6`, `v7`, `v9`. And the remaining `v11`, `v2`, `v4`, `v8` show that their two distributions are sperated from each other, which might be suitable for modeling.


```{r,  fig.height = 5, fig.width = 10}
data %>%
  select(V15:V28, Class) %>% 
  gather(A, B, -Class) %>% 
  ggplot(aes(x = B, y = A, fill = Class)) + 
  geom_density_ridges(alpha = 0.5) +
  scale_x_log10() + 
  ggtitle("Density Plot for each Variable") + 
  xlab("Value") + 
  ylab("Variable Name")
```

So follow the same logic, we find two distributions of following features somehow overlapped: `v15`, `v16`, `v18`, `v22`, `v24`, `v25`, `v26`. And the remaining `v17`, `v19`, `v20`, `v21`, `v23`, `v27`, `v28` show that their two distributions are sperated from each other.

<br>

## **Corrlation Analysis for each feature**

```{r,  fig.height = 5, fig.width = 10}
corr.data = data %>%
  select(-Class) %>% 
  as.matrix() %>% 
  cor()

ggcorrplot(corr.data, hc.order = FALSE, type = "upper",
   tl.srt = 90, lab = FALSE,
   colors = c("#E46726", "white", "#6D9EC1")) + 
  ggtitle("Correlation Pot for Each feature")
```
From the PLot above We know that

- a moderate to strong positive correlation between Amount and V2.
- a weak to moderate positive correlation between Amount and V5.
- a moderate to strong positive correlation between Time and V3.
- a very weak positive correlation between V11, V15, and V25.
- a moderate to strong negative correlation between Amount and V7.
- a very weak negative correlation between Amount and V20.
- There are no significant correlation between either Amount or Time and the rest of the variables (V1, V4, V6, V8, V9, V10, V12, V13, V14, V16, V17, V18, V19, V21, V22, V23, V24, V26, V27, V28). 

The graph above demonstrates that most of the data features are not correlated. As a reminder, most of the features were presented to a `Principal Component Analysis` (PCA) algorithm. The features `V1` to `V28` are most probably the Principal Components resulted after transforming the real features through `PCA.` However, we have no idea what the original features should be.


# **Data Preprocessing**

## **Normalization**
```{r}
ggplot(data = data, aes(x = Amount)) +
  geom_histogram() +
  xlim(0,5) +
  ggtitle("Histogram of Amout(Non-Normalized)") +
  ylab("Number of Transactions")
```

From the histogram above, we can see the distribution is right-skewed and it is not centered around its mean. Therefore, normalization is needed for the variable amount.



```{r}
normalize <- function(x){
      return((x - mean(x, na.rm = TRUE))/sd(x, na.rm = TRUE))
}
data$Amount <- normalize(data$Amount)
```

Here is the distribution of amount after Normalizing

```{r}
ggplot(data = data, aes(x = Amount)) +
  geom_histogram() +
  xlim(0,5) +
  ggtitle("Histogram of Amout(Normalized)") +
  ylab("Number of Transactions")
```

<br>
